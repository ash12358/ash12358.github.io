{"meta":{"title":"Hexo","subtitle":"","description":"Ash的博客","author":"Ash's Blogs","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"word2vec练习","slug":"word2vec练习","date":"2019-11-24T10:52:41.000Z","updated":"2019-11-25T11:29:27.544Z","comments":true,"path":"2019/11/24/word2vec练习/","link":"","permalink":"http://yoursite.com/2019/11/24/word2vec%E7%BB%83%E4%B9%A0/","excerpt":"","text":"原文 pytorch_word2vec_model.py 12345678910111213141516171819202122232425import numpy as npimport torchimport torch.nn as nnimport torch.nn.functional as Fclass SkipGram(nn.Module): def __init__(self, vocab_size, embd_size): super(SkipGram, self).__init__() self.embeddings = nn.Embedding(vocab_size, embd_size) def forward(self, focus, context): embed_focus = self.embeddings(focus) embed_ctx = self.embeddings(context) # score = torch.mm(embed_focus, torch.t(embed_ctx)) score = torch.mul(embed_focus, embed_ctx).sum(dim=1) log_probs = score #F.logsigmoid(score) return log_probs def loss(self, log_probs, target): loss_fn = nn.BCEWithLogitsLoss() # loss_fn = nn.NLLLoss() loss = loss_fn(log_probs, target) return loss pytorch_train.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106import randomimport reimport torchimport torch.optim as optimfrom tqdm import tqdmfrom pytorch_word2vec_model import SkipGramepochs = 50negative_sampling = 4window = 2vocab_size = 1embd_size = 300device = \"cuda\" if torch.cuda.is_available() is True else \"cpu\"def batch_data(x, batch_size=128): in_w = [] out_w = [] target = [] for text in x: for i in range(window, len(text) - window): word_set = set() in_w.append(text[i]) in_w.append(text[i]) in_w.append(text[i]) in_w.append(text[i]) out_w.append(text[i - 2]) out_w.append(text[i - 1]) out_w.append(text[i + 1]) out_w.append(text[i + 2]) target.append(1) target.append(1) target.append(1) target.append(1) # negative sampling count = 0 while count &lt; negative_sampling: rand_id = random.randint(0, vocab_size-1) if not rand_id in word_set: in_w.append(text[i]) out_w.append(rand_id) target.append(0) count += 1 if len(out_w) &gt;= batch_size: yield [in_w, out_w, target] in_w = [] out_w = [] target = [] if out_w: yield [in_w, out_w, target]def train(train_text_id, model,opt): model.train() # 启用dropout和batch normalization ave_loss = 0 pbar = tqdm() cnt=0 for x_batch in batch_data(train_text_id): in_w, out_w, target = x_batch in_w_var = torch.tensor(in_w).to(device) out_w_var = torch.tensor(out_w).to(device) target_var = torch.tensor(target,dtype=torch.float).to(device) model.zero_grad() log_probs = model(in_w_var, out_w_var) loss = model.loss(log_probs, target_var) loss.backward() opt.step() ave_loss += loss.item() pbar.update(1) cnt += 1 pbar.set_description('&lt; loss: %.5f &gt;' % (ave_loss / cnt)) pbar.close()text_id = []vocab_dict = &#123;&#125;with open( 'corpus.txt', encoding='utf-8') as fp: for line in fp: lines = re.sub(\"[^A-Za-z0-9']+\", ' ', line).lower().split() line_id = [] for s in lines: if not s: continue if s not in vocab_dict: vocab_dict[s] = len(vocab_dict) id = vocab_dict[s] line_id.append(id) if id==11500: print(id,s) text_id.append(line_id)vocab_size = len(vocab_dict)print('vocab_size', vocab_size)model = SkipGram(vocab_size, embd_size).to(device)for epoch in range(epochs): print('epoch', epoch) opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, weight_decay=0) train(text_id, model,opt) logits的作用是把$(0,1)$的数值，变到$(-\\infty, \\infty)$。如果是分类，假设是0、1分类，1的概率是$p$，那变化公式是$\\frac{p}{1-p}$，一开始我以为是类似于$\\frac{某个概率}{1-这个概率}$这种形式，但是后来学到Logistic模型，发现好像不是这么回事，假设是N分类，则更像是$\\frac{p_k}{p_N}$，其中$k=1,2,…,N-1$。 代码里BCEWithLogitsLoss()是把$(-\\infty, \\infty)$的数值，变到$(0,1)$，通过sigmoid函数就可以实现。而其实，sigmoid函数就是从这里推导出来的。","categories":[],"tags":[{"name":"word2vec","slug":"word2vec","permalink":"http://yoursite.com/tags/word2vec/"}]},{"title":"EM算法","slug":"EM算法","date":"2019-11-22T14:25:00.000Z","updated":"2019-11-25T11:24:34.070Z","comments":true,"path":"2019/11/22/EM算法/","link":"","permalink":"http://yoursite.com/2019/11/22/EM%E7%AE%97%E6%B3%95/","excerpt":"","text":"EM参考资料就像统计学习方法里讲的一样，从一个三硬币例子讲起。假如有三枚硬币，投掷这三枚硬币正面朝上的概率分别为$\\pi$，$p$，$q$。先投第一枚硬币，如果正面朝上，则接下来投第二枚硬币，然后记录该枚硬币的投掷情况，若正面朝上记为1，反面朝上记为0；如果第一枚硬币是反面朝上，则接下来投第三枚硬币，然后同理记录该枚硬币的投掷情况。问题：求最后0、1数据的分布。假设模型背后的参数是$\\theta$，则就是求$P(x;\\theta)$，其中$x\\in{0,1}$。 极大似然估计如果只是投一枚硬币，假设正面朝上被一个参数$q$指导着。连续投10次，观察到的数据是$[0,1,0,1,0,1,0,1,0,1]$。然后设此10次投掷为一个事件，虽然该事件已经发生了，但假设我们带着现在观察到的数据，回到该事件尚未发生的时刻，然后找一个$q$，使得该事件的结果与我们已经观察到的结果一致。这是一个反推的过程，很好理解，但关键是要用数学表达出这个过程。就这样表达：这个事件就是投10次，这10次投掷的结果分别是$[0,1,0,1,0,1,0,1,0,1]$。则该事件的概率可以表达成：$$P(X=0;q) \\cdot P(X=1;q)\\cdot …\\cdot P(X=1;q)$$需要我们做的工作是找到一个$q$，使得上述公式最大，就可以了。这样的话，可以把上述公式改写成$q$的函数，即：$$l(q)=P(X=0;q) \\cdot P(X=1;q)\\cdot …\\cdot P(X=1;q)$$为了最大化上述公式，可以使用导数这个工具，但观察上述公式，连乘的形式不利于导数的运算，所以在效果不变的前提下，再次改写：$$l(q)=\\mathop{\\log}{[P(X=0;q) \\cdot P(X=1;q)\\cdot …\\cdot P(X=1;q)]}$$也就是：$$l(q)=\\sum_{i=1}^{10}{\\mathop{\\log}{P(X=x_i;q)}}$$根据我们日常生活中的经验，一枚硬币，每一次投掷正面朝上的概率是一样的（上帝掷筛子吗），而这里的这个概率正好可以跟前面讲的$q$关联起来，所以，顺水推舟地，设$$P(X=1)=q\\P(X=0)=1-q$$然后：$$l(q)=\\sum_{i=1}^{10}{\\mathop{\\log}{P(X=x_i)}}=\\mathop{\\log}{q}\\cdot \\mathop{\\log}{(1-q)} \\cdot … \\cdot \\mathop{\\log}{(1-q)}$$最大化这个式子，得出$q=0.5$。一般化的形式：$$l(q)=\\sum_{i=1}^{N}{\\mathop{\\log}{[q^{x_i}\\cdot (1-q)^{1-x_i}]}}$$对$q$求导：$$\\frac{\\partial{l(q)}}{\\partial{q}}=\\sum_{i}^{N}{\\frac{x_i-q}{q(1-q)}}$$令其等于0，解得$q=\\frac{\\sum{x_i}}{N}$。 EM算法回到EM算中当中，设$Z$为第一次投币的结果，这是隐变量，不可观测。则部分观测似然函数是：$$l(\\theta)=\\sum_{i=1}^{N}{\\mathop{\\log{P(x_i;\\theta)}}}$$这个式子写不出像上面$l(q)=\\sum_{i=1}^{N}{\\mathop{\\log}{[q^{x_i}\\cdot (1-q)^{1-x_i}]}}$那样的式子，因为有个$\\pi$在那里档着。假设能观测到$Z$，则完全观测似然函数是：$$l(\\theta)=\\sum_{i=1}^{N}{\\mathop{\\log{\\sum_{z}{P(x_i,z;\\theta)}}}}$$按道理来说，到这就可以用极大似然估计那一套去求$\\theta$的解析解，我估计大前辈们也是这么去做的，但是做的过程中发现有难度，不好做。于是最终就设置了EM这一套，专门用来求解此等问题。Jensen：$$E[f(x)]\\ge f(E[x])$$$\\mathop{\\log}$是凹函数，所以$f(E[x])\\ge E[f(x)]$。就是利用这个方法，把上面似然函数的$\\mathop{\\log}$移到了第二个$\\sum$后面。为了利用Jensen还得设计一下$\\mathop{\\log}$内部的这个函数，把它写成期望的形式。假设$Q(z)$是与$z$有关（但不是针对$z$）的的一个概率密度：$$l(\\theta)=\\sum_{i=1}^{N}{\\mathop{\\log}{\\sum_{z}{Q(z){\\frac{P(x_i,z;\\theta)}{Q(z)}}}}}$$然后运行Jensen不等式：$$l(\\theta)\\ge \\sum_{i=1}^{N}{\\sum_{z}{Q(z)\\mathop{\\log}{\\frac{P(x_i,z;\\theta)}{Q(z)}}}}$$取等号时，随机变量是一个常量，$$\\frac{P(x_i,z;\\theta)}{Q(z)}=c$$所以$$Q(z)=\\frac{P(x_i,z;\\theta)}{c}$$因为之前构造$Q(z)$是给它的设定是概率密度，所以有$$\\sum_{z}{Q(z)}=\\sum_{z}{\\frac{P(x_i,z;\\theta)}{c}}=1$$然后，看到这个上面这个形式，它跟softmax非常像，于是自然而然地$$\\sum_{z}{Q(z)}=\\sum_{z}{\\frac{P(x_i,z;\\theta)}{\\sum_{z}{P(x_i,z;\\theta)}}}=1$$所以$$Q(z)=\\frac{P(x_i,z;\\theta)}{\\sum_{z}{P(x_i,z;\\theta)}}=\\frac{P(x_i,z;\\theta)}{P(x_i;\\theta)}=P(z|x_i;\\theta)$$至此，$Q(z)$就构造出来了。EM算法：E：根据参数$\\theta$计算每个样本属于每个$z$的概率，这个概率就是$Q(z)$M：根据得到的$Q(z)$，求出含有$\\theta$的似然函数的下界并最大化它，得到新的参数$\\theta$。重复，至收敛。“可以看到，从思想上来说，和最开始没什么两样，只不过直接最大化似然函数不好做，曲线救国而已。”","categories":[],"tags":[{"name":"EM算法","slug":"EM算法","permalink":"http://yoursite.com/tags/EM%E7%AE%97%E6%B3%95/"}]},{"title":"github搜索技巧","slug":"github搜索技巧","date":"2019-11-22T14:23:00.000Z","updated":"2019-11-25T11:24:25.485Z","comments":true,"path":"2019/11/22/github搜索技巧/","link":"","permalink":"http://yoursite.com/2019/11/22/github%E6%90%9C%E7%B4%A2%E6%8A%80%E5%B7%A7/","excerpt":"","text":"in:name xxx in:readme xxx in:description xxx stars:&gt;100 forks:&gt;100 language:python pushed:&gt;2019-09-01","categories":[],"tags":[{"name":"github","slug":"github","permalink":"http://yoursite.com/tags/github/"}]},{"title":"03统计学习方法","slug":"03统计学习方法","date":"2019-11-19T12:10:46.000Z","updated":"2019-11-25T11:24:07.422Z","comments":true,"path":"2019/11/19/03统计学习方法/","link":"","permalink":"http://yoursite.com/2019/11/19/03%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/","excerpt":"","text":"第一周从模型的角度 监督学习 无监督学习 半监督学习 强化学习 内容： 概论 感知机 k近邻 朴素贝叶斯 决策树 罗辑回归和最大熵 支持向量机 提升方法 EM算法机器推广 隐马尔可夫模型 条件随机场 总结 梯度下降法 牛顿法和拟牛顿法 拉格朗日对偶性 不同模型的差别在于：模型的假设和损失函数的设计 学习要求： 首先理解模型、算法的适用场景 然后理解模型、算法的逻辑框架 依据自己能力掌握个别推导细节 监督学习的实现步骤 得到一个有限的训练集合 得到模型的假设空间，也就是所有的备选模型 确定模型选择的准则，即学习的策略 实现求解最优模型的算法 通过学习方法选择最优模型 利用学习的最优模型对新数据进行预测或分析 对一个训练集，从里面拿出一下x和y来，这个拿取是随机的，可以用x和y的联合概率分布来表示（原来还有这种理解）。 条件概率分布$P(Y|X)$，预测形式$arg\\mathop{min}\\limits_{y}P(y|x)$ 模型有两种：决策模型： 1F=\\&#123;f|Y=f_\\theta(X),\\theta\\in R^n\\&#125; 条件概率分布： 1F=\\&#123;P|P_&#123;\\theta&#125;(Y|X),\\theta\\in R^n\\&#125; F表示假设空间。举个例子，随机变量x和y，他俩是线性关系，如果用决策模型来描述，就是$Y=a_0+a_1X,\\theta=(a_0,a_1)^T$，如果用条件概率分布来描述，就是$Y\\sim N(a_0+a_1X,\\sigma^2)$，这样就决定了给定一个x的情况下，y服从正态分布由$a_0,a_1$决定。 泛化误差学到的模型是$\\hat{f}$，用这个模型对未知数据预测的误差即为泛化误差： 重点：泛化误差上界对于二分类问题，当假设空间是有限个函数的集合$F=\\{f_1,f_2,\\dots ,f_d\\}$时，对任意一个函数$f\\in F$，至少以概率$1-\\delta $，以下不等式成立： 12R(f)\\leq \\hat&#123;R&#125; (f)+\\varepsilon(d,N,\\delta )\\\\\\varepsilon(d,N,\\delta )=\\sqrt&#123;\\frac&#123;1&#125;&#123;2N&#125;(\\log&#123;d&#125;+\\log&#123;\\frac&#123;1&#125;&#123;\\delta &#125;&#125;)&#125; 证明：霍夫丁不等式（Hoeffding’s inequality）是机器学习的基础理论，通过它可以推导出机器学习在理论上的可行性。（这玩意从没见过。。） MLE：最大似然估计 贝叶斯估计 作业1 感知机模型假设：数据是线性可分的 算法收敛性 作业2 当w和b初始值为0时，eta就没有用了，因为被约去了 k近邻数据不一定是线性可分的。 度量距离：欧氏距离 决策准则：多数表决 模型 度量距离 无穷范数与输入变量相差最大的分量比较小 1、2范数不仅与输入变量相差最大的分量比较小，而且每个分量都要小 k值得选择交叉验证方法 分类决策规则 上图其实就是举手表决法 kd树 作业3 模型的复杂度体现在搜索距离分类点最近的k个样本。K值越小，越容易过拟合 线性扫描是O(n)，但是扫描完后还得选出最大的k个，如果用排序的话就是O(nlogn)。kd树是O(logn) sklearn123456789101112131415161718192021222324252627282930313233343536import numpy as npfrom sklearn.neighbors import KNeighborsClassifierx = np.array([[5, 4],[9, 6],[4, 7],[2, 3],[8, 1],[7, 2]])y = np.array([0, 0, 0, 1, 1, 1])class_label = &#123;0: '正例', 1: '负例'&#125;sample = np.array([[5, 3], [3, 3]])for k in range(1, 7): print('k=&#123;&#125;'.format(k)) classifier = KNeighborsClassifier(n_neighbors=k) classifier.fit(x, y) result = classifier.predict(sample) print(result) for i in range(sample.shape[0]): print('\\t样本:&#123;&#125;, 预测结果:&#123;&#125;'.format(sample[i], class_label[result[i]]))output:k=1 样本:[5 3], 预测结果:正例 样本:[3 3], 预测结果:负例k=2 样本:[5 3], 预测结果:正例 样本:[3 3], 预测结果:正例k=3 样本:[5 3], 预测结果:负例 样本:[3 3], 预测结果:正例k=4 样本:[5 3], 预测结果:负例 样本:[3 3], 预测结果:正例k=5 样本:[5 3], 预测结果:负例 样本:[3 3], 预测结果:负例k=6 样本:[5 3], 预测结果:正例 样本:[3 3], 预测结果:正例 自编程12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import numpy as npclass K(): def __init__(self, k, x, y): self.x = x self.y = y self.k = k def calssifier(self, sample): nums_sample = sample.shape[0] nums_x = self.x.shape[0] # 数据扩充维度，方便利用矩阵运算求距离 _sample = np.stack([sample] * nums_x, axis=1) _x = np.stack([self.x] * nums_sample, axis=0) distance = np.sqrt(np.sum((_sample - _x) ** 2, axis=2)) # 距离排序 distance = np.argsort(distance, axis=-1) # 选择最近的k个 index = distance[:, :self.k] index = np.reshape(index, (-1)) result = self.y[index] result = np.reshape(result, (-1, self.k)) final_result = [] for res in result: final_result.append(np.argmax(np.bincount(res))) final_result = np.array(final_result) return final_resultx = np.array([[5, 4],[9, 6],[4, 7],[2, 3],[8, 1],[7, 2]])y = np.array([0, 0, 0, 1, 1, 1])class_label = &#123;0: '正例', 1: '负例'&#125;sample = np.array([[5, 3], [3, 3]])for k in range(1, 7): print('k=&#123;&#125;'.format(k)) k_classifier = K(k=k, x=x, y=y) result = k_classifier.calssifier(sample=sample) for i in range(sample.shape[0]): print('\\t样本:&#123;&#125;, 预测结果:&#123;&#125;'.format(sample[i], class_label[result[i]]))output:k=1 样本:[5 3], 预测结果:正例 样本:[3 3], 预测结果:负例k=2 样本:[5 3], 预测结果:正例 样本:[3 3], 预测结果:正例k=3 样本:[5 3], 预测结果:负例 样本:[3 3], 预测结果:正例k=4 样本:[5 3], 预测结果:负例 样本:[3 3], 预测结果:正例k=5 样本:[5 3], 预测结果:负例 样本:[3 3], 预测结果:负例k=6 样本:[5 3], 预测结果:正例 样本:[3 3], 预测结果:正例 作业解答为什么极大似然估计里的概率密度函数，而不是概率 其实这里是用f(xi)代替了P(|x-xi|&lt;e)，即xi邻域的概率 贝叶斯估计 concurrent多线程、多进程小例子12345678910111213141516171819from concurrent import futuresimport threadingimport osdef func(i): print(os.getpid(), threading.current_thread(), i) return i**2# with futures.ProcessPoolExecutor(max_workers=5) as executor:with futures.ThreadPoolExecutor(max_workers=5) as executor: # 建立多进程(线程)任务 tasks = [executor.submit(func, i) for i in range(10)] # 驱动多进行运行 done_iter = futures.as_completed(tasks) # 提取运行结果 res = [future.result() for future in done_iter] print(res) 第二周朴素贝叶斯法基于贝叶斯定理和特征条件独立假设的分类方法。 决策函数、条件概率 注意： 在估计该方法的参数时，可以使用极大似然估计法和贝叶斯估计法。这也意味着，不要混淆朴素贝叶斯分类法和贝叶斯估计法。 极大似然估计，上图第二项，有0的可能，所以换成了贝叶斯估计 决策树模型问题：如何选择根节点、如何避免过拟合。 特征选择再次复习，熵是自信息的期望。当事件以1/e概率出现时，熵最大。经验熵：使用有工作作为根节点，分类后的的条件熵是：（条件是有工作）H(D)是经验熵，H(D|A)是给定条件A下D的条件熵。差是增益，也就是熵变小了多少。换句话说，增益是数据混乱减少的程度。意思就是某个特征分的类别越多，混乱程度是越大的，像是某种程度上的正则化。 决策树的生成第四步是预剪枝。 剪枝后剪枝。决策树叶子节点越多，说明模型越复杂，泛化能力低。 CART算法二分树。提出来了一个度量混乱程度的方法——基尼指数选择Gini最小的那个特征。不用考虑Ha(D)了，也就是不用考虑信息增益比了。因为这里是二叉树。 熵、信息增益、基尼指数熵和基尼指数都是衡量混乱程度的，用于离散变量。方差：基本是用于连续变量。内部节点是否剪枝只与该节点为根节点的子树有关！ 第三周对数线性模型logistics 判别模型最大熵 生成模型 logisticlogit变换，把0到1区间变换到$-\\infty$到$\\infty$。就是这么直接。注意：公式里的$\\pi(x)$取值是$[0,1]$。含义是概率。然后反解出$\\pi(x)$。可以看出来了，上面的公式就是凑出来的，但是凑得很好，很紧凑。最终的公式（模型）：其实就是造成了一个公式，把$[-\\infty,\\infty]$变换到$[0,1]$，然后把0到1的这个数看做是概率（赋予它概率的意义）。这个是求w的方法。上面就是在w的条件下，y的分布。通过最大化上面的式子，求w。方法是梯度下降。推导（这个才明白这个分子分母的设置，估计这个设计，解出来好看。也有可能是第一个这么做的人，“误导”了后人）： 最大熵为什么选熵最大的：因为在不知道数据真实分布的情况是，假设各类数据是平均分布（熵最大）是比较合理的。看下面这个例子，一目了然。增加条件：“一弯”表示从样本里得出（观察得到）。对上面的式子求期望，就能得到。下面公式的意思是：让其在总体中出现的概率等于在样本中出现的概率。上面第二行式子的意思：约束条件。条件就是要让熵最大，但是得符合实际情况。求解结果：区别：以前都是直接使用x的取值，而这个最大熵模型是用的x和y的特征函数，也即是$f(x,y)$。 拉格朗日对偶性对应的L函数：下面说明：为什么等价然后，对偶形式：三个定理：定理一：对偶问题提供了一个原始问题解得下界。定理二：给出了强对偶性的充分条件。也就告诉了我们什么时候可以利用对偶求原问题的解。凸优化问题：在凸集上寻找凸函数的的极小值。凸集、凸函数。slater条件：可行域的交集也得是凸集，且有内点。正例：反例:)kkt条件。 改进的迭代尺度法。。。懵了。放弃，进下一章 支持向量机硬间隔最大化函数距离、几何距离使用函数距离的问题。函数距离的意义：当固定下一个超平面，判断不同的点到该超平面的距离。 软间隔 非线性与核函数 序列最小最优化算法两个两个轮换着优化。 $a=b$$$b=a$$","categories":[],"tags":[{"name":"统计学习方法","slug":"统计学习方法","permalink":"http://yoursite.com/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"}]},{"title":"git基本使用","slug":"git基本使用","date":"2019-11-19T10:14:06.000Z","updated":"2019-11-25T11:23:19.448Z","comments":true,"path":"2019/11/19/git基本使用/","link":"","permalink":"http://yoursite.com/2019/11/19/git%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","excerpt":"","text":"理想中的作业流程 在本机上通过pycharm写代码，然后通过git这个工具把代理同步到web端。 然后在服务器上使用git pull下来，然后运行。 如果在服务器端运行的过程中出现了什么问题，再回到本机上修改代码，再push到web端。 然后再在服务器上pull下来，运行。 大体就是这么个想法。 基本使用先通过网页，在github上新建一个仓库。然后可以通过git clone https://xxx.git克隆这个仓库，也可以现在本地做一些工作，然后通过git remote add origin https://xxx.git来关联远程仓库。然后就是些基本命令： git add .将当前工作目录中的所有文件添加到工作区 git commit -m &quot;.&quot; 提交到（我也不知道是哪） git push推到web端 git pulldown都服务器上 其他命令 git branch branch_name新建一个分支 git checkout branch_name切换分支","categories":[],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-11-18T10:18:54.368Z","updated":"2019-11-18T10:18:54.368Z","comments":true,"path":"2019/11/18/hello-world/","link":"","permalink":"http://yoursite.com/2019/11/18/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"220. 存在重复元素 III","slug":"220. 存在重复元素 III","date":"2019-10-01T07:46:00.000Z","updated":"2019-12-01T05:31:17.809Z","comments":true,"path":"2019/10/01/220. 存在重复元素 III/","link":"","permalink":"http://yoursite.com/2019/10/01/220.%20%E5%AD%98%E5%9C%A8%E9%87%8D%E5%A4%8D%E5%85%83%E7%B4%A0%20III/","excerpt":"","text":"220. 存在重复元素 III题目描述给定一个整数数组，判断数组中是否有两个不同的索引 i 和 j，使得 nums [i] 和 nums [j] 的差的绝对值最大为 t，并且 i 和 j 之间的差的绝对值最大为 ķ。 来源：力扣（LeetCode）链接：https://leetcode-cn.com/problems/contains-duplicate-iii著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 树状数组树状数组的通俗解释树状数组本身是一个数组，只不过把它画成树的形式，为了更好理解接下来的操作。下面来结合图片进行说明。首先假设有一个原始数组$A$，$A=[1,2,3,4,5,6,7,8]$。图下图所示。 然后，再申请一个数组$C$，长度和$A$相同，初始状态$C$为空，即$C=[0,0,0,0,0,0,0,0]$。如下图所示。这个$C$数组就是树状数组，可能现在还不不太清楚，怎么就树状数组了，别急，现在我们把$C$数组在形式上稍微改变一下，再和$A$数组对应起来，如下图所示。现在在看，是不是$C$数组就像是一个树了，帅气智。然后需要了解的就是这个$C$数组究竟是用来干嘛的。还是看个例子。$C[1]=A[1]\\C[2]=C[1]+A[2]=A[1]+A[2]\\C[3]=A[3]=A[1]+A[2]+A[3]\\C[4]=C[2]+C[3]+A[4]=A[1]+A[2]+A[3]+A[4]\\C[5]=A[5]\\C[6]=C[5]+A[6]=A[5]+A[6]\\C[7]=A[7]\\C[8]=C[4]+C[6]+C[7]+A[8]=A[1]+A[2]+A[3]+A[4]+A[5]+A[6]+A[7]+A[8]\\$这里直接给出计算$C[i]$的公式，$C[i]=sum{A[j]|i-2^k+1\\leq j \\leq i}\\ (k是i的二进制表示中末尾0的个数)$。其实从上图也能看出来。我们把数字都标上，就变成了下图。然后，需要知道的就要 这个数组数组可以用来做什么。它能做的事有两个： $sum(i)$，计算$A[1]+A[2]+…+A[i]$； $add(i, x)$, 让$A[i]$加上$x$，然后更新更个树状数组。 下面先说$sum(i)$。$sum(i)=A[1]+A[2]+…+A[i]\\=A[1]+A[2]+A[i-2^k]+A[i-2^k+1]+…+A[i]\\=sum(i-2^k)+C[i]\\=sum(i-lowbit(i))+C[i]$其中$lowbit(i)$是用来计算$2^k$的，其中$k$是$i$的二进制表示中末尾0的个数。这里不解释了，直接上代码，用的就是$lowbit(i)$这个函数。 123int lowbit(int x) &#123; return x &amp; -x;&#125; 然后是$sum(i)$的代码实现： 1234567int sum(int x) &#123; int s = 0; for (int i = x; i &gt; 0; i -= lowbit(i)) &#123; s += c[i]; &#125; return s;&#125; 说完$sum(i)$再说$add(i, x)$。$add(i, x)$其实就是$A[i]=A[i]+x$，其实就是看改变$A[i]$的时候会改变那些$C[j]$。跟$sum(i)$很像，只不过是反着来。下面是代码。 12345void add(int i, int x) &#123; for (int k = i; k &lt;= n; k += lowbit(k)) &#123; c[k] += x; &#125;&#125; 树状数组就先说这些，下面是树状数组的模版。 1234567891011121314151617181920static const int n = 20060;int c[n+1];int lowbit(int x) &#123; return x &amp; -x;&#125;void add(int x, int flag) &#123; for (int i = x; i &lt;= n; i += lowbit(i)) &#123; c[i] += flag; &#125;&#125;int sum(int x) &#123; int s = 0; for (int i = x; i &gt; 0; i -= lowbit(i)) &#123; s += c[i]; &#125; return s;&#125; 解题思路构造一个长度为$k$滑动窗口，然后利用树状数组查找这个滑动窗口中小于等于某个值的元素的个数。举个例子。输入: nums = [1,0,1,1], k = 1, t = 2输出: true构造一个长度为1的滑动窗口，把0号加入到滑动窗口，此时滑动窗口为[1]，然后处理2号数字，也就是0。因为要满足nums [i] 和 nums [j] 的差的绝对值最大为 t，也即是$|nums[j]-0|\\leq t$,也就是$-t+0\\leq nums[j]\\leq t+0$，也就是$-2\\leq nums[j]\\leq 2$，然后树状数组登场，查找当前滑动窗口中小于等于$t+0$的元素的个数，也就是小于等于2的元素的个数，[1]中小于等于2的元素有1个；然后查找当前滑动窗口中小于等于$(-t+0)-1$的元素的个数（想想为啥要再减1），也就是小于等于-3的元素的个数，[1]中小于等于-3的元素有0个；所以[1]中大于等于2并且小于等于2的元素有1个。又因为这是在当前滑动窗口里找的，所以可定符合 i 和 j 之间的差的绝对值最大为 ķ。如果在处理j号元素时，没有找到符合条件的元素，也就是让滑动窗口右移一位，也即是把$nums[j]$加入到滑动窗口，并把$nums[j-k]$移除滑动窗口。代码就是 12add(nums[j], 1); #表示滑动窗口中多了一个值为nums[j]的元素add(nums[j-k], -1); #表示滑动窗口中少了一个值为nums[j-k]的元素 前提是$j-k\\geq 0$。以上就是答题思路，然而在具体实践中，常遇到的两个问题是 求左右边界时有可能超int范围，要用long类型 对数组进行离散化处理。再利用一个数组a，关键代码如下：12345for (int i = 0; i &lt; nums.size(); i++) &#123; a[i] = nums[i];&#125;sort(a, a+nums.size());#排序an = unique(a, a + nums.size()) - a; #a的长度 在更新树状数组是，不再直接用$nums[i]$，然是用$nums[i]$在a数组中的下标。代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class Solution &#123;public: static const int n = 20060; int c[n+1]; int a[n+1]; int an; int lowbit(int x) &#123; return x &amp; -x; &#125; void add(int x, int flag) &#123; for (int i = x; i &lt;= n; i += lowbit(i)) &#123; c[i] += flag; &#125; &#125; int sum(int x) &#123; int s = 0; for (int i = x; i &gt; 0; i -= lowbit(i)) &#123; s += c[i]; &#125; return s; &#125; bool containsNearbyAlmostDuplicate(vector&lt;int&gt;&amp; nums, int k, int t) &#123; memset(c, 0, sizeof(c)); memset(a, 0, sizeof(a)); for (int i = 0; i &lt; nums.size(); i++) &#123; a[i] = nums[i]; &#125; sort(a, a+nums.size()); an = unique(a, a + nums.size()) - a; for (int j = 0; j &lt; nums.size(); j++) &#123; long long int left = (long long int )-t + (long long int )nums[j]-1, right = (long long int )t + (long long int )nums[j]; int idx_left = lower_bound(a, a+an, left) - a; int idx_right = lower_bound(a, a+an, right) - a; if (a[idx_left] == left) idx_left++; if (a[idx_right] == right) idx_right++; if (sum(idx_right) - sum(idx_left) &gt; 0) &#123; return true; &#125; int idx_j = lower_bound(a, a+an, nums[j]) - a + 1; add(idx_j, 1); int i = j - k; if (i &gt;= 0) &#123; int idx_i = lower_bound(a, a+an, nums[i]) - a + 1; add(idx_i, -1); &#125; &#125; return false; &#125;&#125;;","categories":[],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/tags/leetcode/"},{"name":"树状数组","slug":"树状数组","permalink":"http://yoursite.com/tags/%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84/"}]}]}